\section{Related Work}

There are two main techniques in hand gesture recognition - appearance based and model based. These are akin to probabilistic models of classification and generative models of classification respectively. \textbf{Appearance based} approaches read the pixels from the camera and build classifiers to label that as belonging to a finite set of classes. The main limitation of this technique is that the set of labels is finite and fixed ahead of time. The advantage of appearance based approaches is their implementation is often extremely fast and therefore suited for situations where live classification is important. Examples of appearance based techniques can be found in \cite{shotton2011, wang2009}. \textbf{Model based} approaches start with a set of hypotheses of the final classification based on rules of the object being classified. For instance, in the case of hand gestures a hypothesis can be a particular orientation of the joints.  An advantage here is that the hypotheses can be generated from an infinite space of possible classifications. The main disadvantage of model-based techniques is that they are often computationally expensive. In addition, model-based approaches tend to be very complex. An example of a model based technique can be found in \cite{oikonomidis2011}.

As advance sensor technology has become more accessible, many developers and researchers are studying natural interfaces. \cite{stuerzlinger2010} overviews a variety of input devices for interfacing with 3D models including mouse designs with six degrees of freedom, haptics devices for simulating realistic forces, and computer vision based techniques for head and hand tracking. Very relevant to our project, \cite{hoffman2010} presents a method for identifying 25 gestures in 3D using the gyroscope and accelerometer in the Nintendo Wii remote. This falls in a more general category called ``spatially convenient input-devices''. 

The work most similar to ours is from Microsoft \cite{shotton2011} on the details of the Kinect's appearance-based skeleton recognition algorithm. They classify each pixel as belonging to some portion of the body and then pool all pixels from each portion to determine the joint positions. Their training set is generated from synthetic 3D images of body orientations. The feature extraction technique and the random forest classification algorithm used in this work is borrowed from \cite{lepetit2005}. Our project differs from this in the method for generating training samples, and the scale of classification. 

\cite{wang2009} present another appearance-based method for detecting hand gestures using just an RGB camera. Their solution requires the user wear a glove with a special pattern imprinted on it. The camera pictures the hand in different orientations, normalizes the image, and identifies the nearest neighbor in a database of common hand gestures. Unlike this approach, our technique only uses a color glove for training purposes. 

\cite{oikonomidis2011} present a model-based approach using the Kinect. They first generate a set of hypotheses with knowledge of inverse kinematics, and approximate shape of fingers and hands. They evaluate these hypotheses and pick the most likely one based on the depth image from the Kinect. This technique is able to detect hand gestures even in the place of occlusions and is implemented efficiently reaching up to 15 updates per second. However, being model-based, this technique may be vulnerable to differences in hand shapes, and still lags behind in speed compared to appearance-based techniques. 
